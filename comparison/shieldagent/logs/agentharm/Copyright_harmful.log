Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:02<?, ?it/s]
Traceback (most recent call last):
  File "/home/yx/yx_search/aisafety/DEFEND/comparison/shieldagent/evaluate.py", line 527, in <module>
    main()
  File "/home/yx/yx_search/aisafety/DEFEND/comparison/shieldagent/evaluate.py", line 432, in main
    evaluator = ShieldAgentEvaluator(model_path=args.model_path)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/yx/yx_search/aisafety/DEFEND/comparison/shieldagent/evaluate.py", line 16, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/yx/anaconda3/envs/defend/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/yx/anaconda3/envs/defend/lib/python3.11/site-packages/transformers/modeling_utils.py", line 262, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/yx/anaconda3/envs/defend/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4319, in from_pretrained
    ) = cls._load_pretrained_model(
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/yx/anaconda3/envs/defend/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4897, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/yx/anaconda3/envs/defend/lib/python3.11/site-packages/transformers/modeling_utils.py", line 896, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/yx/anaconda3/envs/defend/lib/python3.11/site-packages/accelerate/utils/modeling.py", line 343, in set_module_tensor_to_device
    new_value = value.to(device, non_blocking=non_blocking)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 23.55 GiB of which 8.31 MiB is free. Process 3528857 has 462.00 MiB memory in use. Process 2053941 has 2.12 GiB memory in use. Including non-PyTorch memory, this process has 2.12 GiB memory in use. Process 2053948 has 2.12 GiB memory in use. Process 2053950 has 1.72 GiB memory in use. Process 2053952 has 1.97 GiB memory in use. Process 2053953 has 1.27 GiB memory in use. Process 2053946 has 1.27 GiB memory in use. Process 2053942 has 1.27 GiB memory in use. Process 2053955 has 1.27 GiB memory in use. Process 2053951 has 1.27 GiB memory in use. Process 2053944 has 1.27 GiB memory in use. Process 2053945 has 1.27 GiB memory in use. Process 2053943 has 1.27 GiB memory in use. Process 2053954 has 1.27 GiB memory in use. Process 2053947 has 1.27 GiB memory in use. Process 2053949 has 256.00 MiB memory in use. Of the allocated memory 1.86 GiB is allocated by PyTorch, and 15.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
