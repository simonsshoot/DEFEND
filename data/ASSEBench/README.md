<p align="center">
  <img src="assets/logo.png" alt="AgentAuditor Logo" width="150"/>
</p>

<h3 align="center">ğŸ•µï¸ AgentAuditor: Human-Level Safety and Security Evaluation for LLM Agents</h3>

<p align="center">
  <a href="https://arxiv.org/abs/2506.00641">ğŸ“œ Paper</a> |
  <a href="https://github.com/Astarojth/AgentAuditor-ASSEBench/tree/main/AgentAuditor">ğŸ“š Dataset</a> |
  <a href="#-quick-start">ğŸš€ Quick Start</a>
</p>

<p align="center">
  <img src="https://visitor-badge.laobi.icu/badge?page_id=Astarojth.AgentAuditor-ASSEBench" alt="Visitor Badge" />
  <img src="https://img.shields.io/github/stars/Astarojth/AgentAuditor-ASSEBench?style=social" alt="GitHub Stars" />
  <img src="https://img.shields.io/badge/license-Apache%202.0-blue.svg" alt="License" />
  <img src="https://img.shields.io/badge/python-3.8+-blue.svg" alt="Python Version" />
</p>

---

> **AgentAuditor** is a universal, training-free, memory-augmented reasoning framework that empowers LLM evaluators to emulate human expert evaluators in identifying safety and security risks in LLM agent interactions.

## ğŸ’¥ News

- **[2025-10-01]** Our paper is accpeted by NIPS 2025! ğŸš€
- **[2025-06-03]** We release the AgentAuditor paper along with dataset! ğŸš€


## ğŸ“Œ Table of Contents

- [ğŸ” Overview](#-overview)
- [âœ¨ Key Features](#-key-features)
- [ğŸ—ï¸ Architecture](#ï¸-architecture)
- [ğŸ“ Repository Structure](#-repository-structure)
- [ğŸ§ª Pipeline Overview](#-pipeline-overview)
- [âš¡ Quick Start](#-quick-start)
- [ğŸ“„ Citation](#-citation)
- [ğŸ“¬ Contact](#-contact)


## ğŸ” Overview

As Large Language Model (LLM)-based agents become increasingly autonomous, they introduce new safety and security risks that traditional evaluation methods struggle to detect. **AgentAuditor** addresses this critical challenge through a novel approach that combines experiential memory with human-like reasoning.

### The Problem
- **Autonomous LLM agents** are deployed in high-stakes scenarios (finance, healthcare, critical infrastructure)
- **Traditional evaluation methods** fail to capture nuanced safety and security risks
- **Human-level assessment** is needed but expensive and doesn't scale

### Our Solution
AgentAuditor introduces a sophisticated evaluation framework that:

1. **ğŸ§  Builds Experiential Memory**: Constructs a structured knowledge base from past agent interactions, extracting semantic features (scenario, risk, behavior) and generating Chain-of-Thought (CoT) reasoning traces

2. **ğŸ” Employs Smart Retrieval**: Uses multi-stage, context-aware retrieval-augmented generation (RAG) to guide LLM evaluators with relevant historical experiences

3. **ğŸ“Š Introduces ASSEBench**: The first comprehensive benchmark specifically designed to evaluate how well LLM-based evaluators can identify both safety risks and security threats in agent interactions

## âœ¨ Key Features

- **ğŸš€ Training-Free**: No model fine-tuning required - works with any LLM evaluator
- **ğŸ¯ Human-Level Performance**: Achieves expert-level accuracy in safety assessment  
- **ğŸ“ˆ Scalable**: Automated pipeline that scales to large datasets
- **ğŸ”§ Modular Design**: Each component can be used independently or as part of the full pipeline
- **ğŸŒ Universal**: Works across different agent types and interaction scenarios
- **ğŸ“Š Comprehensive**: Evaluates both safety risks and security threats

## ğŸ—ï¸ Architecture

![Overall Architecture](assets/agent_auditor_overview.png)

## ğŸ“ Repository Structure

```
AgentAuditor-ASSEBench/
â”œâ”€â”€ ğŸ“„ README.md                        # Project documentation
â”œâ”€â”€ ğŸ“„ LICENSE                          # Apache 2.0 license
â”œâ”€â”€ ğŸ“„ requirements.txt                  # Python dependencies
â”œâ”€â”€ ğŸš€ agent_auditor.sh                 # Main AgentAuditor pipeline script
â”œâ”€â”€ ğŸ¯ direct_eval.sh                   # Direct evaluation baseline script
â”œâ”€â”€ ğŸ¨ assets/                          # Visual assets
â”‚   â”œâ”€â”€ logo.png                        # Project logo
â”‚   â””â”€â”€ agent_auditor_overview.png      # Architecture diagram
â”œâ”€â”€ ğŸ”§ AgentAuditor/                    # Core framework implementation
â”‚   â”œâ”€â”€ __init__.py                     # Package initialization
â”‚   â”œâ”€â”€ __main__.py                     # CLI entry point (python -m AgentAuditor)
â”‚   â”œâ”€â”€ ğŸ“Š data/                        # Training and configuration data
â”‚   â”‚   â”œâ”€â”€ agentharm.json              # AgentHarm dataset
â”‚   â”‚   â”œâ”€â”€ AgentJudge-*.json           # AgentJudge dataset variants
â”‚   â”‚   â”œâ”€â”€ rjudge.json                 # RJudge dataset
â”‚   â”‚   â””â”€â”€ fewshot.txt                 # Few-shot CoT examples
â”‚   â”œâ”€â”€ âš™ï¸ params/                      # Pre-computed parameters
â”‚   â”‚   â”œâ”€â”€ clus_param.pkl              # Clustering parameters (FINCH)
â”‚   â”‚   â””â”€â”€ infer_param.pkl             # Inference parameters (embeddings)
â”‚   â””â”€â”€ ğŸ› ï¸ tasks/                       # Pipeline task implementations
â”‚       â”œâ”€â”€ preprocess.py               # LLM-based semantic annotation
â”‚       â”œâ”€â”€ cluster.py                  # Weighted clustering with FINCH
â”‚       â”œâ”€â”€ demo.py                     # CoT demonstration generation
â”‚       â”œâ”€â”€ demo_repair.py              # CoT validation and repair
â”‚       â”œâ”€â”€ infer_emb.py                # Embedding-based retrieval
â”‚       â”œâ”€â”€ infer.py                    # Few-shot inference engine
â”‚       â”œâ”€â”€ infer_fix1.py               # JSON parsing and correction
â”‚       â”œâ”€â”€ infer_fix2.py               # LLM-based output refinement
â”‚       â”œâ”€â”€ eval.py                     # Performance evaluation
â”‚       â”œâ”€â”€ direct_eval.py              # Zero-shot baseline evaluation
â”‚       â””â”€â”€ direct_metric.py            # Baseline metrics calculation
â””â”€â”€ ğŸ“Š ASSEBench/                       # Benchmark dataset
    â”œâ”€â”€ category/                       # Risk categorization
    â”‚   â”œâ”€â”€ safety/                     # Safety-related scenarios
    â”‚   â”‚   â”œâ”€â”€ f.json                  # Failure cases
    â”‚   â”‚   â”œâ”€â”€ r.json                  # Risk cases  
    â”‚   â”‚   â””â”€â”€ s.json                  # Success cases
    â”‚   â””â”€â”€ security/                   # Security-related scenarios
    â”‚       â”œâ”€â”€ f.json                  # Failure cases
    â”‚       â”œâ”€â”€ r.json                  # Risk cases
    â”‚       â””â”€â”€ s.json                  # Success cases
    â””â”€â”€ dataset/                        # Evaluation datasets
        â”œâ”€â”€ AgentJudge-loose.json       # Loose evaluation criteria
        â”œâ”€â”€ AgentJudge-safety.json      # Safety-focused evaluation
        â”œâ”€â”€ AgentJudge-security.json    # Security-focused evaluation
        â””â”€â”€ AgentJudge-strict.json      # Strict evaluation criteria
```


## ğŸ§ª Pipeline Overview

AgentAuditor employs a sophisticated multi-stage pipeline that mimics human expert evaluation. The framework transforms raw agent interactions into structured assessments through memory-augmented reasoning.

### ğŸ”„ Main AgentAuditor Pipeline

The complete pipeline consists of six sequential stages:

| Stage | Component | Purpose | Input | Output |
|-------|-----------|---------|--------|---------|
| **1ï¸âƒ£ Preprocessing** | `preprocess.py` | Semantic annotation using LLM | Raw agent interactions | Structured memory with scenario/risk/behavior tags |
| **2ï¸âƒ£ Clustering** | `cluster.py` | Identify representative cases | Annotated interactions | Cluster representatives for demonstrations |
| **3ï¸âƒ£ Demo Generation** | `demo.py` + `demo_repair.py` | Create & validate CoT examples | Representative cases | High-quality CoT demonstrations |
| **4ï¸âƒ£ Retrieval** | `infer_emb.py` | Find relevant experiences | Test cases + demo pool | Test cases + similar examples |
| **5ï¸âƒ£ Inference** | `infer.py` + `infer_fix*.py` | Few-shot evaluation with CoT | Augmented test cases | Safety predictions with reasoning |
| **6ï¸âƒ£ Evaluation** | `eval.py` | Performance analysis | Predictions vs ground truth | Metrics (Accuracy, F1, etc.) |

### ğŸ¯ Direct Evaluation Baseline

For comparison without AgentAuditor enhancement:

| Component | Purpose | Method |
|-----------|---------|---------|
| `direct_eval.py` | Zero-shot safety evaluation | Direct LLM assessment |
| `direct_metric.py` | Baseline performance metrics | Standard evaluation metrics |

### ğŸ’¡ Usage Patterns

**ğŸš€ Complete Pipeline:**
```bash
bash agent_auditor.sh
```

**ğŸ”§ Individual Stages:**
```bash
# Semantic annotation
python -m AgentAuditor rjudge preprocess  

# Find representative cases
python -m AgentAuditor rjudge cluster     

# Generate demonstrations
python -m AgentAuditor rjudge demo        

# Embedding-based retrieval
python -m AgentAuditor rjudge infer_emb   

# Few-shot inference
python -m AgentAuditor rjudge infer       

# Calculate final metrics
python -m AgentAuditor rjudge eval        
```

**ğŸ“Š Baseline Comparison:**
```bash
bash direct_eval.sh
```

## âš¡ Quick Start

### 1. Clone and Setup

```bash
# Clone the repository
git clone https://github.com/Astarojth/AgentAuditor-ASSEBench.git
cd AgentAuditor-ASSEBench

# Install dependencies
pip install -r requirements.txt
```

### 2. Configure API Keys

You need to configure your OpenAI API key in multiple locations. Look for `GPTConfig` classes throughout the codebase and update the `API_KEY` field:

```python
# Example configuration in Python files
class GPTConfig:
    API_KEY = "your-openai-api-key-here"
    MODEL = "gpt-4"  # or your preferred model
    # ... other settings
```

### 3. Run Evaluation

Choose your evaluation approach:

**ğŸ¯ Quick Evaluation (Full Pipeline):**
```bash
# Run complete AgentAuditor evaluation
bash agent_auditor.sh

# Run baseline comparison
bash direct_eval.sh
```

**ğŸ”§ Custom Dataset Evaluation:**
```bash
# Replace 'rjudge' with your dataset name
python -m AgentAuditor your_dataset preprocess
python -m AgentAuditor your_dataset cluster
python -m AgentAuditor your_dataset demo
python -m AgentAuditor your_dataset infer_emb
python -m AgentAuditor your_dataset infer
python -m AgentAuditor your_dataset eval
```

### 4. View Results

Results will be saved in the respective output directories. Key metrics include:
- **Accuracy**: Overall correctness of safety assessments
- **Precision/Recall**: Fine-grained performance analysis
- **F1-Score**: Balanced performance measure

## ğŸ“„ Citation

If you use AgentAuditor or ASSEBench in your research, please cite our paper:

```bibtex
@article{luo2025agentauditor,
  title={AgentAuditor: Human-Level Safety and Security Evaluation for LLM Agents}, 
  author={Hanjun Luo and Shenyu Dai and Chiming Ni and Xinfeng Li and Guibin Zhang and Kun Wang and Tongliang Liu and Hanan Salam},
  journal={arXiv preprint arXiv:2506.00641},
  year={2025},
  url={https://arxiv.org/abs/2506.00641}
}
```

## ğŸ“¬ Contact

For questions, collaborations, or support:

**Primary Contacts:**
- **Hanjun Luo**: [hanjunluowork@gmail.com](mailto:hanjunluowork@gmail.com)
- **Chiming Ni**: [chimingni@gmail.com](mailto:chimingni@gmail.com)

**Project Resources:**
- ğŸ“„ **Paper**: [ArXiv Link](https://arxiv.org/abs/2506.00641)
- ğŸ› **Issues**: [GitHub Issues](https://github.com/Astarojth/AgentAuditor-ASSEBench/issues)
- ğŸ’¬ **Discussions**: [GitHub Discussions](https://github.com/Astarojth/AgentAuditor-ASSEBench/discussions)
- ğŸŒ **Project Page**: *Coming Soon*

---

<p align="center">
  <strong>Made with â¤ï¸ for safer AI agents</strong><br/>
  <sub>AgentAuditor Â© 2025 - Licensed under Apache 2.0</sub>
</p>
